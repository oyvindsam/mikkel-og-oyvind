{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team CoronaBoost\n",
    "477631,  \n",
    "Ã˜yvind Samuelsen, Mikkel Nygard\n",
    "Challenge: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train/test data\n",
    "train = pd.read_csv('data/challenge3_train.csv', index_col='id')\n",
    "test = pd.read_csv('data/challenge3_test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper lists for column types\n",
    "features_nom_low = ['f5'] # nominal low cardinality (<=3)\n",
    "features_nom_high = ['f12', 'f28'] # >= 26\n",
    "features_ord_alph = ['f15', 'f20']\n",
    "\n",
    "# columns to perform one hot encoding on \n",
    "ohe_columns = features_ord_alph + features_nom_low + features_nom_high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "Based on exploration we have done these steps for data cleaning. After each step we tested to see if the accuracy in a basic XGB model improved, to decide if we would include it or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix 0 value noise, change to most common value in column. \n",
    "impute_0_columns = ['f3', 'f18', 'f21']\n",
    "\n",
    "for column in impute_0_columns:\n",
    "    train.loc[train[column] == 0, column] = train[column].mode() # mode() finds most common value\n",
    "    test.loc[test[column] == 0, column] = test[column].mode()\n",
    "\n",
    "# remove -1 from 'month' column f11\n",
    "train.loc[train['f11'] == -1, 'f11'] = train['f11'].mode()\n",
    "test.loc[test['f11'] == -1, 'f11'] = test['f11'].mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Similar to data cleaning, we have tested each step to see if it improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a positive correlation for target 1 between f6 and f25, which is not present in target 0.\n",
    "train['f6_f25'] = train['f6'].fillna(1)**2*train['f25'].fillna(1)**2\n",
    "test['f6_f25'] = test['f6'].fillna(1)**2*test['f25'].fillna(1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We hypothesise column f11 describes months, from 0-11\n",
    "# so add cyclical feature\n",
    "def cyc_enc(df, col, max_vals):\n",
    "    df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_vals)\n",
    "    df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_vals)\n",
    "    return df\n",
    "train = cyc_enc(train, 'f11', 11)\n",
    "test = cyc_enc(test, 'f11', 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For columns with dtype 'object' we need to represent them as numeric.\n",
    "# One hot encoding gave us best result, compared to label encoding and target encoding.\n",
    "train = pd.get_dummies(train, columns=ohe_columns)\n",
    "test = pd.get_dummies(test, columns=ohe_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "We use XGBoost, with optimized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train set into a train and test set\n",
    "X = train.drop(['target'], axis=1)\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training final model, predict test set and delivery csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 14s, sys: 2.63 s, total: 11min 17s\n",
      "Wall time: 3min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Fit model with best parameters over all training data. Then predict test data.\n",
    "# Parameters were found by using Hyperopt, as detailed in long document.\n",
    "xgbmodel = XGBRegressor(n_estimators=300, \n",
    "            scale_pos_weight=len(y[y==0]) / len(y[y==1]),        \n",
    "            **{'colsample_bytree': 0.7541691347538767, \n",
    "          'eval_metric': 'auc', \n",
    "          'gamma': 0.4511313995198146, \n",
    "          'learning_rate': 0.03432377692457561, \n",
    "          'max_depth': 9, \n",
    "          'min_child_weight': 4.0,\n",
    "          'reg_alpha': 0.33753466341724986,\n",
    "          'reg_lambda': 0.3322614438833425, \n",
    "          'subsample': 0.8})\n",
    "\n",
    "xgbmodel.fit(X, y, verbose=False)\n",
    "predictions = xgbmodel.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test.index,\n",
    "    'target': predictions,\n",
    "}).to_csv('477631+477578_predictions.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
